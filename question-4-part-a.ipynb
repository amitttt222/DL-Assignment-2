{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n#import tensorflow as tf\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nimport torch\nimport wandb\nwandb.login(key='e595ff5b95c353a42c4bd1f35b70856d4309ef00')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndata_dir = '/kaggle/input/inaturalist12k/Data/inaturalist_12K'\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n# Define the CNN model architecture\nclass CNN(nn.Module):\n    def __init__(self, num_filters, filter_size, activation_fn, filter_org, batch_norm, dropout,num_neuron_fc,filter_size_org):\n        super(CNN, self).__init__()\n        if(activation_fn==\"ReLU\"):\n            activation_fn=nn.ReLU\n        if(activation_fn==\"GELU\"):\n            activation_fn=nn.GELU\n        if(activation_fn==\"SiLU\"):\n            activation_fn=nn.SiLU\n        if(activation_fn==\"Mish\"):\n            activation_fn=Mish\n        if(filter_org==\"double\"):\n            filter_org=[1,2,2,2,2]\n        if(filter_org==\"same\"):\n            filter_org=[1,1,1,1,1]\n        if(filter_org==\"half\"):\n            filter_org=[1,0.5,0.5,0.5,0.5]\n        if(filter_size_org=='same'):\n            filter_size_org=[1,1,1,1,1]\n        if(filter_size_org==\"double\"):\n            filter_size_org=[1,2,2,2,2]\n        if(filter_size_org==\"half\"):\n            filter_size_org=[1,0.5,0.5,0.5,0.5]\n        layers = []\n        in_channels = 3\n        w=256\n        for i, f in enumerate(filter_org):\n            out_channels = int(num_filters * f)\n            filter_size1 = int(filter_size*filter_size_org[i])\n            #calculate feature map dimension\n            w=int((w-filter_size1+(2*1))+1)\n            w=int(((w-2)//2)+1)\n            #ends\n            layers.append(nn.Conv2d(int(in_channels), int(out_channels), int(filter_size1), padding=1))\n            if batch_norm:\n                layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(activation_fn())\n            layers.append(nn.MaxPool2d(2))\n            if dropout > 0:\n                layers.append(nn.Dropout(dropout))\n            in_channels = out_channels\n        #print(w)\n        self.cnn = nn.Sequential(*layers)      \n        self.fc1 = nn.Linear(int(out_channels) * w * w, num_neuron_fc)\n        self.fc2 = nn.Linear(num_neuron_fc, 10)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\nsweep_config = {\n    'method': 'random', #grid, random,#bayes\n    'name' : 'Random_sweep_cross_entropy',\n    'metric': {\n      'name': 'test accuracy',\n      'goal': 'maximize'  \n    },\n    'parameters': {\n        'num_filters': {\n            'values': [32]\n        },\n        'filter_size':{\n            'values':[5]\n        },\n         'activation_fn':{\n            'values':[\"GELU\"]\n        },\n        'filter_org':{\n            'values':[\"same\"]\n        },\n         'filter_size_org':{\n            'values':[\"same\"]\n        },\n         'batch_norm': {\n            'values': [False]\n        },\n        'dropout': {\n            'values': [0.3]\n        },\n         'num_neuron_fc':{\n            'values':[20]\n        },\n        'data_augmentation': {\n            'values': [\"yes\"]\n        },  \n        \n        \n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assign_2')\nfrom types import SimpleNamespace\ndef main():\n    with wandb.init() as run:\n        params={}\n        params=dict(wandb.config)\n        params=SimpleNamespace(**params)\n        run_name=\"num_filter_\"+str(wandb.config.num_filters)+\"-filter_size_\"+str(wandb.config.filter_size)+\"-activation_fn_\"+wandb.config.activation_fn\\\n                + \"-filter_org\"+wandb.config.filter_org+\"-filter_size_org\"+wandb.config.filter_size_org+\"-batch_norm\"+str(wandb.config.batch_norm)\\\n                + \"-dropout\"+str(wandb.config.dropout)+\"num_neuron_fc\"+str(wandb.config.num_neuron_fc)+\"-data_augmentation\"+wandb.config.data_augmentation\n        wandb.run.name=run_name\n        \n        #data augmentation function starts\n        def get_transforms(data_augmentation):\n            if data_augmentation==\"yes\":\n                transform = transforms.Compose([\n                    transforms.RandomHorizontalFlip(p=0.5),\n                    transforms.RandomVerticalFlip(p=0.5),\n                    transforms.RandomRotation(degrees=30),\n                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n                    transforms.Resize((256,256)),\n                    transforms.ToTensor()\n                ])\n            else:\n                transform = transforms.Compose([\n                    transforms.Resize((256,256)),\n                    transforms.ToTensor()\n                ])\n            return transform\n        #data augmentation function ends\n        \n        def print_image_label(imgs,targets,output):\n            print(len(imgs),len(targets),len(output))\n            imgs=imgs.cpu()\n            targets=targets.cpu()\n            out=output.cpu().numpy()\n            \n            preds=np.argmax(out,axis=1)\n            print(len(preds))\n            fig,axis=plt.subplots(10,3,figsize=(10,20),constrained_layout=True)\n            idx=0\n            for i in range(10):\n                for j in range(3):\n                    axis[i][j].imshow(imgs[idx].permute(1,2,0))\n                    axis[i][j].set_xlabel(\"Predicted \"+str(preds[idx]))\n                    axis[i][j].set_ylabel(\"Actual \"+str(targets[idx].item()))\n                    idx+=1\n            wandb.log({'Predicting on test data':wandb.Image(plt)})\n            plt.title(\"Predicting test data\")\n            plt.show()\n                    \n                    \n            \n            \n        \n        #transform and splitting data starts\n        transform = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n        train_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/train',transform=get_transforms(wandb.config.data_augmentation))\n        test_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/val',transform=transform)\n\n        val_size = int(len(train_dataset) * 0.9)\n        train_size = len(train_dataset) - val_size\n\n        train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n        train_loader=DataLoader(train_dataset,batch_size=30,shuffle=True)\n        val_loader=DataLoader(val_dataset,batch_size=30,shuffle=False)\n        test_loader=DataLoader(test_dataset,batch_size=30,shuffle=True)\n        #transform and splitting data ends\n        \n        # Initialize the model\n        model = CNN(params.num_filters, params.filter_size, params.activation_fn, params.filter_org, params.batch_norm, params.dropout, params.num_neuron_fc,params.filter_size_org).to(device)\n\n        # Define the loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n        # Train the model\n        num_epochs = 10\n        for epoch in range(num_epochs):\n            model.train()\n            for batch_idx, (data, target) in enumerate(train_loader):\n                data, target = data.to(device), target.to(device)# Move data and target to the device\n                optimizer.zero_grad()\n                output = model(data)\n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n\n            # Evaluate the model on train_loader\n            model.eval()\n            train_correct = 0\n            train_total = 0\n            train_loss = 0.0\n            with torch.no_grad():\n                for data, target in train_loader:\n                    data, target = data.to(device), target.to(device)# Move data and target to the device\n                    output = model(data)\n                    loss = criterion(output, target)\n                    train_loss += loss.item()\n                    _, predicted = torch.max(output.data, 1)\n                    train_total += target.size(0)\n                    train_correct += (predicted == target).sum().item()\n\n            train_accuracy = 100 * train_correct/train_total\n            train_loss /= len(train_loader)\n\n            # Set model to evaluation mode\n            model.eval()\n            test_loss = 0.0\n            correct = 0\n            total = 0\n            flag=False\n            with torch.no_grad():\n                for batch_idx, (data, target) in enumerate(test_loader):\n                    data, target = data.to(device), target.to(device)# Move data and target to the device\n                    output = model(data)\n                    if (flag==False and epoch==9):\n                        print_image_label(data,target,output)\n                        flag=True\n            \n                    loss = criterion(output, target)\n                    test_loss += loss.item()\n                    _, predicted = torch.max(output.data, 1)\n                    total += target.size(0)\n                    correct += (predicted == target).sum().item()\n\n            # Calculate validation metrics\n            test_loss /= len(test_loader)\n            test_accuracy = 100 * correct / total\n\n            # Print metrics for current epoch\n            #print('Epoch: {} \\t Training Loss: {:.6f}\n            print(\"epoch\",epoch)\n            print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n            print('Validation Loss: {:.6f} \\t test Accuracy: {:.6f}'.format(test_loss, test_accuracy))\n            wandb.log({'train loss':train_loss,'train accuracy':train_accuracy,'test loss':test_loss,'test accuracy':test_accuracy})\n        \nwandb.agent(sweep_id, function=main,count=1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-12T19:52:29.723368Z","iopub.execute_input":"2023-04-12T19:52:29.724263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}