{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from PIL import Image\n# import numpy as np\n\n# def load_data(data_dir):\n#     images = []\n#     labels = []\n#     classes = os.listdir(data_dir)\n#     for cls in classes:\n#         cls_path = os.path.join(data_dir, cls)\n#         if os.path.isdir(cls_path):\n#             cls_images = os.listdir(cls_path)\n#             for img_name in cls_images:\n#                 img_path = os.path.join(cls_path, img_name)\n#                 img = Image.open(img_path).resize((224, 224))\n#                 img = np.array(img) / 255.0\n#                 images.append(img)\n#                 labels.append(cls)\n#     return np.array(images), np.array(labels)\n\n\n# data_dir = '/kaggle/input/inaturalist12k/Data/inaturalist_12K'\n\n# train_dir = os.path.join(data_dir, 'train')\n# X_train, y_train = load_data(train_dir)\n\n# val_dir = os.path.join(data_dir, 'val')\n# X_val, y_val = load_data(val_dir)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T14:53:51.721586Z","iopub.execute_input":"2023-04-08T14:53:51.722367Z","iopub.status.idle":"2023-04-08T14:53:51.727606Z","shell.execute_reply.started":"2023-04-08T14:53:51.722326Z","shell.execute_reply":"2023-04-08T14:53:51.726248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(os.listdir('/kaggle/input'))","metadata":{"execution":{"iopub.status.busy":"2023-04-08T14:53:58.079943Z","iopub.execute_input":"2023-04-08T14:53:58.080324Z","iopub.status.idle":"2023-04-08T14:53:58.085170Z","shell.execute_reply.started":"2023-04-08T14:53:58.080289Z","shell.execute_reply":"2023-04-08T14:53:58.083970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\n#import tensorflow as tf\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nimport torch\nimport wandb\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndata_dir = '/kaggle/input/inaturalist12k/Data/inaturalist_12K'\n#-------------------------------------------------------------------------------------------------------\ndef get_transforms(data_augmentation):\n    if data_augmentation==\"yes\":\n        transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomRotation(degrees=30),\n            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n            transforms.Resize((256,256)),\n            transforms.ToTensor()\n        ])\n    else:\n        transform = transforms.Compose([\n            transforms.Resize((256,256)),\n            transforms.ToTensor()\n        ])\n    return transform\n\ntransform = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\ntrain_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/train',transform=get_transforms(\"yes\"))\ntest_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/val',transform=transform)\n\nval_size = int(len(train_dataset) * 0.2)\ntrain_size = len(train_dataset) - val_size\n\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ntrain_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\nval_loader=DataLoader(val_dataset,batch_size=16,shuffle=False)\ntest_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n#--------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2023-04-10T22:48:42.535799Z","iopub.execute_input":"2023-04-10T22:48:42.536566Z","iopub.status.idle":"2023-04-10T22:48:44.048070Z","shell.execute_reply.started":"2023-04-10T22:48:42.536520Z","shell.execute_reply":"2023-04-10T22:48:44.047041Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n# Define the CNN model architecture\nclass CNN(nn.Module):\n    def __init__(self, num_filters, filter_size, activation_fn, filter_org, batch_norm, dropout,num_neuron_fc,filter_size_org):\n        super(CNN, self).__init__()\n        if(activation_fn==\"ReLU\"):\n            activation_fn=nn.ReLU\n        if(activation_fn==\"GELU\"):\n            activation_fn=nn.GELU\n        if(activation_fn==\"SiLU\"):\n            activation_fn=nn.SiLU\n        if(activation_fn==\"Mish\"):\n            activation_fn=Mish\n        if(filter_org==\"double\"):\n            filter_org=[1,2,2,2,2]\n        if(filter_org==\"same\"):\n            filter_org=[1,1,1,1,1]\n        if(filter_org==\"half\"):\n            filter_org=[1,0.5,0.5,0.5,0.5]\n        if(filter_size_org=='same'):\n            filter_size_org=[1,1,1,1,1]\n        if(filter_size_org==\"double\"):\n            filter_size_org=[1,2,2,2,2]\n        if(filter_size_org==\"half\"):\n            filter_size_org=[1,0.5,0.5,0.5,0.5]\n        layers = []\n        in_channels = 3\n        w=256\n        for i, f in enumerate(filter_org):\n            out_channels = int(num_filters * f)\n            filter_size1 = int(filter_size*filter_size_org[i])\n            #calculate feature map dimension\n            w=int((w-filter_size1+(2*1))+1)\n            w=int(((w-2)//2)+1)\n            #ends\n            layers.append(nn.Conv2d(int(in_channels), int(out_channels), int(filter_size1), padding=1))\n            if batch_norm:\n                layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(activation_fn())\n            layers.append(nn.MaxPool2d(2))\n            if dropout > 0:\n                layers.append(nn.Dropout(dropout))\n            in_channels = out_channels\n        #print(w)\n        self.cnn = nn.Sequential(*layers)      \n        self.fc1 = nn.Linear(int(out_channels) * w * w, num_neuron_fc)\n        self.fc2 = nn.Linear(num_neuron_fc, 10)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\n\n\n# #------------------------------------------------------------------------------------------------------\n# sweep_config = {\n#     'method': 'random', #grid, random,#bayes\n#     'name' : 'Random_sweep_cross_entropy',\n#     'metric': {\n#       'name': 'valid accuracy',\n#       'goal': 'maximize'  \n#     },\n#     'parameters': {\n#         'num_filters': {\n#             'values': [32,64,96]\n#         },\n#         'filter_size':{\n#             'values':[3,5,7,2]\n#         },\n#          'activation_fn':{\n#             'values':[\"Mish\",\"ReLU\",\"GELU\",\"SiLU\"]\n#         },\n#         'filter_org':{\n#             'values':[\"same\",\"double\",\"half\"]\n#         },\n#          'filter_size_org':{\n#             'values':[\"same\",\"double\"]\n#         },\n#          'batch_norm': {\n#             'values': [True,False]\n#         },\n#         'dropout': {\n#             'values': [0.3,0.2]\n#         },\n#          'num_neuron_fc':{\n#             'values':[10,20]\n#         },\n#         'data_augmentation': {\n#             'values': [\"no\",\"yes\"]\n#         }, \n# #         'loss': {\n# #             'values': ['entropy_loss']\n# #         }, \n        \n        \n#     }\n# }\n\n# sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assign_2')\n# from types import SimpleNamespace\n# def main():\n#     with wandb.init() as run:\n#         params={}\n#         params=dict(wandb.config)\n#         params=SimpleNamespace(**params)\n#         run_name=\"num_filter_\"+str(wandb.config.num_filters)+\"-filter_size_\"+str(wandb.config.filter_size)+\"-activation_fn_\"+wandb.config.activation_fn\\\n#                 + \"-filter_org\"+wandb.config.filter_org+\"-filter_size_org\"+wandb.config.filter_size_org+\"-batch_norm\"+str(wandb.config.batch_norm)\\\n#                 + \"-dropout\"+str(wandb.config.dropout)+\"num_neuron_fc\"+str(wandb.config.num_neuron_fc)+\"-data_augmentation\"+wandb.config.data_augmentation\n#         wandb.run.name=run_name\n        \n#         #data augmentation function starts\n#         def get_transforms(data_augmentation):\n#             if data_augmentation==\"yes\":\n#                 transform = transforms.Compose([\n#                     transforms.RandomHorizontalFlip(p=0.5),\n#                     transforms.RandomVerticalFlip(p=0.5),\n#                     transforms.RandomRotation(degrees=30),\n#                     transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n#                     transforms.Resize((256,256)),\n#                     transforms.ToTensor()\n#                 ])\n#             else:\n#                 transform = transforms.Compose([\n#                     transforms.Resize((256,256)),\n#                     transforms.ToTensor()\n#                 ])\n#             return transform\n#         #data augmentation function ends\n        \n#         #transform and splitting data starts\n#         transform = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n#         train_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/train',transform=get_transforms(wandb.config.data_augmentation))\n#         test_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/val',transform=transform)\n\n#         val_size = int(len(train_dataset) * 0.2)\n#         train_size = len(train_dataset) - val_size\n\n#         train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n#         train_loader=DataLoader(train_dataset,batch_size=64,shuffle=True)\n#         val_loader=DataLoader(val_dataset,batch_size=64,shuffle=False)\n#         test_loader=DataLoader(test_dataset,batch_size=64,shuffle=False)\n#         #transform and splitting data ends\n        \n#         # Initialize the model\n#         model = CNN(params.num_filters, params.filter_size, params.activation_fn, params.filter_org, params.batch_norm, params.dropout, params.num_neuron_fc,params.filter_size_org).to(device)\n\n#         # Define the loss function and optimizer\n#         criterion = nn.CrossEntropyLoss()\n#         optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n#         # Train the model\n#         num_epochs = 10\n#         for epoch in range(num_epochs):\n#             model.train()\n#             for batch_idx, (data, target) in enumerate(train_loader):\n#                 data, target = data.to(device), target.to(device)# Move data and target to the device\n#                 optimizer.zero_grad()\n#                 output = model(data)\n#                 loss = criterion(output, target)\n#                 loss.backward()\n#                 optimizer.step()\n\n#             # Evaluate the model on train_loader\n#             model.eval()\n#             train_correct = 0\n#             train_total = 0\n#             train_loss = 0.0\n#             with torch.no_grad():\n#                 for data, target in train_loader:\n#                     data, target = data.to(device), target.to(device)# Move data and target to the device\n#                     output = model(data)\n#                     loss = criterion(output, target)\n#                     train_loss += loss.item()\n#                     _, predicted = torch.max(output.data, 1)\n#                     train_total += target.size(0)\n#                     train_correct += (predicted == target).sum().item()\n\n#             train_accuracy = 100 * train_correct/train_total\n#             train_loss /= len(train_loader)\n\n#             # Set model to evaluation mode\n#             model.eval()\n#             val_loss = 0.0\n#             correct = 0\n#             total = 0\n#             with torch.no_grad():\n#                 for batch_idx, (data, target) in enumerate(val_loader):\n#                     data, target = data.to(device), target.to(device)# Move data and target to the device\n#                     output = model(data)\n#                     loss = criterion(output, target)\n#                     val_loss += loss.item()\n#                     _, predicted = torch.max(output.data, 1)\n#                     total += target.size(0)\n#                     correct += (predicted == target).sum().item()\n\n#             # Calculate validation metrics\n#             val_loss /= len(val_loader)\n#             val_accuracy = 100 * correct / total\n\n#             # Print metrics for current epoch\n#             #print('Epoch: {} \\t Training Loss: {:.6f}\n#             print(\"epoch\",epoch)\n#             print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n#             print('Validation Loss: {:.6f} \\t Validation Accuracy: {:.6f}'.format(val_loss, val_accuracy))\n#             wandb.log({'train loss':train_loss,'train accuracy':train_accuracy,'valid loss':val_loss,'valid accuracy':val_accuracy})\n# #         input_size=784\n# #         output_size=10\n# #         obj=neural_network(input_size,output_size,params)\n# #         obj.predict()\n\n    \n# wandb.agent(sweep_id, function=main,count=35)\n\n#----------------------------------------------------------------------------------------------------\n\n\n#Define the model hyperparameters\nnum_filters = 96\nfilter_size = 5\nactivation_fn = \"ReLU\"\nfilter_org = \"same\"\nfilter_size_org=\"double\" # extra added\nbatch_norm = False\ndropout = 0.3\nnum_neuron_fc=10\n\n\n# Initialize the model\nmodel = CNN(num_filters, filter_size, activation_fn, filter_org, batch_norm, dropout, num_neuron_fc,filter_size_org).to(device)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nnum_epochs = 1\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)# Move data and target to the device\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n    \n    # Evaluate the model on train_loader\n    model.eval()\n    train_correct = 0\n    train_total = 0\n    train_loss = 0.0\n    with torch.no_grad():\n        for data, target in train_loader:\n            data, target = data.to(device), target.to(device)# Move data and target to the device\n            output = model(data)\n            loss = criterion(output, target)\n            train_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            train_total += target.size(0)\n            train_correct += (predicted == target).sum().item()\n\n    train_accuracy = 100 * train_correct/train_total\n    train_loss /= len(train_loader)\n    \n    # Set model to evaluation mode\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(val_loader):\n            data, target = data.to(device), target.to(device)# Move data and target to the device\n            output = model(data)\n            loss = criterion(output, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(output.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            \n    # Calculate validation metrics\n    val_loss /= len(val_loader)\n    val_accuracy = 100 * correct / total\n    \n    # Print metrics for current epoch\n    #print('Epoch: {} \\t Training Loss: {:.6f}\n    print(\"epoch\",epoch)\n          \n    print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n    print('Validation Loss: {:.6f} \\t Validation Accuracy: {:.6f}'.format(val_loss, val_accuracy))\n\n\n\n\n#------------------------------------------------------------------------------------------\n    \n    \n    \n    \n    \n#     train_loss = 0.0\n#     for batch_idx, (data, target) in enumerate(train_loader): \n#         data, target = data.to(device), target.to(device)# Move data and target to the device\n#         optimizer.zero_grad()\n#         output = model(data)\n#         loss = criterion(output, target)\n#         loss.backward()\n#         optimizer.step()\n#         train_loss += loss.item()\n\n#     print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss/len(train_loader)))\n    \n#     # Evaluate the model on train_loader\n#     model.eval()\n#     train_correct = 0\n#     train_total = 0\n#     train_loss = 0.0\n#     with torch.no_grad():\n#         for data, target in train_loader:\n#             output = model(data)\n#             loss = criterion(output, target)\n#             train_loss += loss.item()\n#             _, predicted = torch.max(output.data, 1)\n#             train_total += target.size(0)\n#             train_correct += (predicted == target).sum().item()\n\n#     train_accuracy = train_correct/train_total\n#     train_loss /= len(train_loader)\n\n#     # Evaluate the model on val_loader\n#     model.eval()\n#     val_correct = 0\n#     val_total = 0\n#     val_loss = 0.0\n#     with torch.no_grad():\n#         for data, target in val_loader:\n#             output = model(data)\n#             loss = criterion(output, target)\n#             val_loss += loss.item()\n#             _, predicted = torch.max(output.data, 1)\n#             val_total += target.size(0)\n#             val_correct += (predicted == target).sum().item()\n\n#     val_accuracy = val_correct/val_total\n#     val_loss /= len(val_loader)\n\n#     print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n#     print('Validation Loss: {:.6f} \\t Validation Accuracy: {:.6f}'.format(val_loss, val_accuracy))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T22:48:46.043398Z","iopub.execute_input":"2023-04-10T22:48:46.044268Z","iopub.status.idle":"2023-04-10T22:48:48.195801Z","shell.execute_reply.started":"2023-04-10T22:48:46.044198Z","shell.execute_reply":"2023-04-10T22:48:48.194245Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1114251552.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# Move data and target to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1114251552.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (23 x 23). Kernel size: (40 x 40). Kernel size can't be greater than actual input size"],"ename":"RuntimeError","evalue":"Calculated padded input size per channel: (23 x 23). Kernel size: (40 x 40). Kernel size can't be greater than actual input size","output_type":"error"}]},{"cell_type":"code","source":"# import torch\n# import matplotlib.pyplot as plt\n\n# # Load an image into a PyTorch tensor\n# for i in range(1):\n#     img=train_loader[i][0].numpy().transpose((1,2,0))\n    \n# # img_tensor = torch.randn(3, 256, 256)\n\n# # # Convert the tensor to a NumPy array\n# # img = img_tensor.numpy().transpose((1, 2, 0))\n\n# plt.imshow(grid.permute(1, 2, 0))\n\n# # Display the image\n#     plt.imshow(img)\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T18:07:24.931478Z","iopub.execute_input":"2023-04-09T18:07:24.932180Z","iopub.status.idle":"2023-04-09T18:07:24.937175Z","shell.execute_reply.started":"2023-04-09T18:07:24.932144Z","shell.execute_reply":"2023-04-09T18:07:24.935623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from PIL import Image\n# import numpy as np\n# #import tensorflow as tf\n# import torchvision\n# import torchvision.transforms as transforms\n# from torch.utils.data import DataLoader\n# from torch.utils.data import random_split\n# import torch\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# data_dir = '/kaggle/input/inaturalist12k/Data/inaturalist_12K'\n# transform = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n\n# train_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/train',transform=transform)\n# test_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/val',transform=transform)\n\n# val_size = int(len(train_dataset) * 0.5)\n# train_size = len(train_dataset) - val_size\n\n# train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# train_loader=DataLoader(train_dataset,batch_size=64,shuffle=True)\n# val_loader=DataLoader(val_dataset,batch_size=64,shuffle=False)\n# test_loader=DataLoader(test_dataset,batch_size=64,shuffle=False)\n\n\n\n\n\n\n# import torch.nn as nn\n# import torch.optim as optim\n\n# # Define the CNN model architecture\n# class CNN(nn.Module):\n#     def __init__(self, num_filters, filter_size, activation_fn, filter_org, batch_norm, dropout):\n#         super(CNN, self).__init__()\n#         layers = []\n#         in_channels = 3\n#         for i, f in enumerate(filter_org):\n#             out_channels = num_filters * f\n#             layers.append(nn.Conv2d(in_channels, out_channels, filter_size, padding=1))\n#             if batch_norm:\n#                 layers.append(nn.BatchNorm2d(out_channels))\n#             layers.append(activation_fn())\n#             layers.append(nn.MaxPool2d(2))\n#             if dropout > 0:\n#                 layers.append(nn.Dropout(dropout))\n#             in_channels = out_channels\n#         self.cnn = nn.Sequential(*layers)\n#         self.fc = nn.Linear(out_channels * 32 * 32, 10)\n\n#     def forward(self, x):\n#         x = self.cnn(x)\n#         x = x.view(x.size(0), -1)\n#         x = self.fc(x)\n#         return x\n\n# # Define the model hyperparameters\n# num_filters = 32\n# filter_size = 3\n# activation_fn = nn.ReLU\n# filter_org = [1,2,2]\n# batch_norm = True\n# dropout = 0.3\n\n# # Initialize the model\n# model = CNN(num_filters, filter_size, activation_fn, filter_org, batch_norm, dropout).to(device)\n\n# # Define the loss function and optimizer\n# criterion = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# # Train the model\n# num_epochs = 5\n# for epoch in range(num_epochs):\n#     model.train()\n#     train_loss = 0.0\n#     correct = 0\n#     total = 0\n#     for batch_idx, (data, target) in enumerate(train_loader):\n#         data, target = data.to(device), target.to(device)# Move data and target to the device\n#         optimizer.zero_grad()\n#         output = model(data)\n#         loss = criterion(output, target)\n#         loss.backward()\n#         optimizer.step()\n#         train_loss += loss.item()\n#         _, predicted = torch.max(output.data, 1)\n#         total += target.size(0)\n#         correct += (predicted == target).sum().item()\n        \n#     # Calculate training metrics\n#     train_loss /= len(train_loader)\n#     train_accuracy = 100 * correct / total\n    \n#     # Set model to evaluation mode\n#     model.eval()\n#     val_loss = 0.0\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for batch_idx, (data, target) in enumerate(val_loader):\n#             data, target = data.to(device), target.to(device)# Move data and target to the device\n#             output = model(data)\n#             loss = criterion(output, target)\n#             val_loss += loss.item()\n#             _, predicted = torch.max(output.data, 1)\n#             total += target.size(0)\n#             correct += (predicted == target).sum().item()\n            \n#     # Calculate validation metrics\n#     val_loss /= len(val_loader)\n#     val_accuracy = 100 * correct / total\n    \n#     # Print metrics for current epoch\n#     #print('Epoch: {} \\t Training Loss: {:.6f}\n#     print(\"epoch\",epoch)\n          \n#     print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n#     print('Validation Loss: {:.6f} \\t Validation Accuracy: {:.6f}'.format(val_loss, val_accuracy))\n\n\n\n\n\n    \n    \n    \n    \n    \n# #     train_loss = 0.0\n# #     for batch_idx, (data, target) in enumerate(train_loader): \n# #         data, target = data.to(device), target.to(device)# Move data and target to the device\n# #         optimizer.zero_grad()\n# #         output = model(data)\n# #         loss = criterion(output, target)\n# #         loss.backward()\n# #         optimizer.step()\n# #         train_loss += loss.item()\n\n# #     print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss/len(train_loader)))\n    \n# #     # Evaluate the model on train_loader\n# #     model.eval()\n# #     train_correct = 0\n# #     train_total = 0\n# #     train_loss = 0.0\n# #     with torch.no_grad():\n# #         for data, target in train_loader:\n# #             output = model(data)\n# #             loss = criterion(output, target)\n# #             train_loss += loss.item()\n# #             _, predicted = torch.max(output.data, 1)\n# #             train_total += target.size(0)\n# #             train_correct += (predicted == target).sum().item()\n\n# #     train_accuracy = train_correct/train_total\n# #     train_loss /= len(train_loader)\n\n# #     # Evaluate the model on val_loader\n# #     model.eval()\n# #     val_correct = 0\n# #     val_total = 0\n# #     val_loss = 0.0\n# #     with torch.no_grad():\n# #         for data, target in val_loader:\n# #             output = model(data)\n# #             loss = criterion(output, target)\n# #             val_loss += loss.item()\n# #             _, predicted = torch.max(output.data, 1)\n# #             val_total += target.size(0)\n# #             val_correct += (predicted == target).sum().item()\n\n# #     val_accuracy = val_correct/val_total\n# #     val_loss /= len(val_loader)\n\n# #     print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n# #     print('Validation Loss: {:.6f} \\t Validation Accuracy: {:.6f}'.format(val_loss, val_accuracy))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T15:37:56.476412Z","iopub.execute_input":"2023-04-08T15:37:56.476829Z","iopub.status.idle":"2023-04-08T15:37:58.123794Z","shell.execute_reply.started":"2023-04-08T15:37:56.476794Z","shell.execute_reply":"2023-04-08T15:37:58.122340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}