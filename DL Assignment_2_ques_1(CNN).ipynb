{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport numpy as np\n#import tensorflow as tf\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\nimport torch\nimport wandb\nwandb.login(key='e595ff5b95c353a42c4bd1f35b70856d4309ef00')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndata_dir = '/kaggle/input/inaturalist12k/Data/inaturalist_12K'","metadata":{"execution":{"iopub.status.busy":"2023-04-10T23:32:23.502881Z","iopub.execute_input":"2023-04-10T23:32:23.503373Z","iopub.status.idle":"2023-04-10T23:32:26.973201Z","shell.execute_reply.started":"2023-04-10T23:32:23.503329Z","shell.execute_reply":"2023-04-10T23:32:26.971949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n# Define the CNN model architecture\nclass CNN(nn.Module):\n    def __init__(self, num_filters, filter_size, activation_fn, filter_org, batch_norm, dropout,num_neuron_fc,filter_size_org):\n        super(CNN, self).__init__()\n        if(activation_fn==\"ReLU\"):\n            activation_fn=nn.ReLU\n        if(activation_fn==\"GELU\"):\n            activation_fn=nn.GELU\n        if(activation_fn==\"SiLU\"):\n            activation_fn=nn.SiLU\n        if(activation_fn==\"Mish\"):\n            activation_fn=Mish\n        if(filter_org==\"double\"):\n            filter_org=[1,2,2,2,2]\n        if(filter_org==\"same\"):\n            filter_org=[1,1,1,1,1]\n        if(filter_org==\"half\"):\n            filter_org=[1,0.5,0.5,0.5,0.5]\n        if(filter_size_org=='same'):\n            filter_size_org=[1,1,1,1,1]\n        if(filter_size_org==\"double\"):\n            filter_size_org=[1,2,2,2,2]\n        if(filter_size_org==\"half\"):\n            filter_size_org=[1,0.5,0.5,0.5,0.5]\n        layers = []\n        in_channels = 3\n        w=256\n        for i, f in enumerate(filter_org):\n            out_channels = int(num_filters * f)\n            filter_size1 = int(filter_size*filter_size_org[i])\n            #calculate feature map dimension\n            w=int((w-filter_size1+(2*1))+1)\n            w=int(((w-2)//2)+1)\n            #ends\n            layers.append(nn.Conv2d(int(in_channels), int(out_channels), int(filter_size1), padding=1))\n            if batch_norm:\n                layers.append(nn.BatchNorm2d(out_channels))\n            layers.append(activation_fn())\n            layers.append(nn.MaxPool2d(2))\n            if dropout > 0:\n                layers.append(nn.Dropout(dropout))\n            in_channels = out_channels\n        #print(w)\n        self.cnn = nn.Sequential(*layers)      \n        self.fc1 = nn.Linear(int(out_channels) * w * w, num_neuron_fc)\n        self.fc2 = nn.Linear(num_neuron_fc, 10)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\nsweep_config = {\n    'method': 'random', #grid, random,#bayes\n    'name' : 'Random_sweep_cross_entropy',\n    'metric': {\n      'name': 'valid accuracy',\n      'goal': 'maximize'  \n    },\n    'parameters': {\n        'num_filters': {\n            'values': [32,64,96]\n        },\n        'filter_size':{\n            'values':[3,5,7,2]\n        },\n         'activation_fn':{\n            'values':[\"Mish\",\"ReLU\",\"GELU\",\"SiLU\"]\n        },\n        'filter_org':{\n            'values':[\"same\",\"double\",\"half\"]\n        },\n         'filter_size_org':{\n            'values':[\"same\",\"double\"]\n        },\n         'batch_norm': {\n            'values': [True,False]\n        },\n        'dropout': {\n            'values': [0.3,0.2]\n        },\n         'num_neuron_fc':{\n            'values':[10,20]\n        },\n        'data_augmentation': {\n            'values': [\"no\",\"yes\"]\n        }, \n        \n        \n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assign_2')\nfrom types import SimpleNamespace\ndef main():\n    with wandb.init() as run:\n        params={}\n        params=dict(wandb.config)\n        params=SimpleNamespace(**params)\n        run_name=\"num_filter_\"+str(wandb.config.num_filters)+\"-filter_size_\"+str(wandb.config.filter_size)+\"-activation_fn_\"+wandb.config.activation_fn\\\n                + \"-filter_org\"+wandb.config.filter_org+\"-filter_size_org\"+wandb.config.filter_size_org+\"-batch_norm\"+str(wandb.config.batch_norm)\\\n                + \"-dropout\"+str(wandb.config.dropout)+\"num_neuron_fc\"+str(wandb.config.num_neuron_fc)+\"-data_augmentation\"+wandb.config.data_augmentation\n        wandb.run.name=run_name\n        \n        #data augmentation function starts\n        def get_transforms(data_augmentation):\n            if data_augmentation==\"yes\":\n                transform = transforms.Compose([\n                    transforms.RandomHorizontalFlip(p=0.5),\n                    transforms.RandomVerticalFlip(p=0.5),\n                    transforms.RandomRotation(degrees=30),\n                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n                    transforms.Resize((256,256)),\n                    transforms.ToTensor()\n                ])\n            else:\n                transform = transforms.Compose([\n                    transforms.Resize((256,256)),\n                    transforms.ToTensor()\n                ])\n            return transform\n        #data augmentation function ends\n        \n        #transform and splitting data starts\n        transform = transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n        train_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/train',transform=get_transforms(wandb.config.data_augmentation))\n        test_dataset=torchvision.datasets.ImageFolder(root=data_dir+'/val',transform=transform)\n\n        val_size = int(len(train_dataset) * 0.2)\n        train_size = len(train_dataset) - val_size\n\n        train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n        train_loader=DataLoader(train_dataset,batch_size=64,shuffle=True)\n        val_loader=DataLoader(val_dataset,batch_size=64,shuffle=False)\n        test_loader=DataLoader(test_dataset,batch_size=64,shuffle=False)\n        #transform and splitting data ends\n        \n        # Initialize the model\n        model = CNN(params.num_filters, params.filter_size, params.activation_fn, params.filter_org, params.batch_norm, params.dropout, params.num_neuron_fc,params.filter_size_org).to(device)\n\n        # Define the loss function and optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n        # Train the model\n        num_epochs = 10\n        for epoch in range(num_epochs):\n            model.train()\n            for batch_idx, (data, target) in enumerate(train_loader):\n                data, target = data.to(device), target.to(device)# Move data and target to the device\n                optimizer.zero_grad()\n                output = model(data)\n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n\n            # Evaluate the model on train_loader\n            model.eval()\n            train_correct = 0\n            train_total = 0\n            train_loss = 0.0\n            with torch.no_grad():\n                for data, target in train_loader:\n                    data, target = data.to(device), target.to(device)# Move data and target to the device\n                    output = model(data)\n                    loss = criterion(output, target)\n                    train_loss += loss.item()\n                    _, predicted = torch.max(output.data, 1)\n                    train_total += target.size(0)\n                    train_correct += (predicted == target).sum().item()\n\n            train_accuracy = 100 * train_correct/train_total\n            train_loss /= len(train_loader)\n\n            # Set model to evaluation mode\n            model.eval()\n            val_loss = 0.0\n            correct = 0\n            total = 0\n            with torch.no_grad():\n                for batch_idx, (data, target) in enumerate(val_loader):\n                    data, target = data.to(device), target.to(device)# Move data and target to the device\n                    output = model(data)\n                    loss = criterion(output, target)\n                    val_loss += loss.item()\n                    _, predicted = torch.max(output.data, 1)\n                    total += target.size(0)\n                    correct += (predicted == target).sum().item()\n\n            # Calculate validation metrics\n            val_loss /= len(val_loader)\n            val_accuracy = 100 * correct / total\n\n            # Print metrics for current epoch\n            #print('Epoch: {} \\t Training Loss: {:.6f}\n            print(\"epoch\",epoch)\n            print('Training Loss: {:.6f} \\t Training Accuracy: {:.6f}'.format(train_loss, train_accuracy))\n            print('Validation Loss: {:.6f} \\t Validation Accuracy: {:.6f}'.format(val_loss, val_accuracy))\n            wandb.log({'train loss':train_loss,'train accuracy':train_accuracy,'valid loss':val_loss,'valid accuracy':val_accuracy})\n\n    \nwandb.agent(sweep_id, function=main,count=35)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-10T23:32:29.590012Z","iopub.execute_input":"2023-04-10T23:32:29.590400Z"},"trusted":true},"execution_count":null,"outputs":[]}]}